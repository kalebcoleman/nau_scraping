"""
This script scrapes course information from the NAU academic catalog.

It uses Selenium to automate a web browser, iterating through a predefined list
of course prefixes and academic terms. For each combination, it fetches a list
of courses, scrapes detailed information from each course page, and stores the
results in a CSV file.

The script is designed to be resumable, loading existing data from the CSV
and skipping courses that have already been scraped unless the `--overwrite`
flag is provided.

Key functionalities:
- Scrapes course details for specified academic terms.
- Handles pagination and dynamic content loading.
- Saves data to a CSV file (`nau_courses.csv`).
- Logs course prefixes that yield no results (`nau_empty_prefixes.csv`).
- Supports headless (default) and headed browser modes for scraping.
- Provides an option to overwrite existing data.
"""
import re
import time
import csv
import sys
import os
from dataclasses import dataclass, asdict
from typing import Optional, List, Dict

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# =========================
# CONFIG
# =========================

BASE = "https://catalog.nau.edu/Courses"

# Dictionary of academic terms to scrape with their corresponding term codes.
TERM_CODES = {
    "Fall 2025": 1257,
    "Spring 2026": 1261,
}

CSV_PATH = "nau_courses.csv"
EMPTY_PREFIXES_CSV = "nau_empty_prefixes.csv"

# Command-line arguments
OVERWRITE = "--overwrite" in sys.argv
HEADLESS = "--no-headless" not in sys.argv

# Time to wait between requests to be polite to the server.
SLEEP_TIME = 0.25

# =========================
# COURSE PREFIXES (NAU)
# =========================

# This list was generated by `course_prefix.py`
PREFIXES = [
    "ACC", "ACM", "ADM", "ADV", "AHB", "AIS", "ANS", "ANT", "APMS", "ARB", "ARE",
    "ARH", "ART", "AS", "ASN", "AST", "AT", "BA", "BASW", "BBA", "BE", "BIO",
    "BME", "BSC", "BSCI", "BUS", "CAL", "CCHE", "CCJ", "CCS", "CENE", "CENS",
    "CHI", "CHM", "CIE", "CINE", "CIT", "CLA", "CM", "CMF", "COM", "CPP", "CS",
    "CSD", "CST", "CTE", "CYB", "DH", "DIS", "ECI", "ECO", "EDF", "EDL", "EDR",
    "EDU", "EE", "EES", "EET", "EGR", "EIT", "EMF", "EMGT", "ENG", "ENT", "ENTY",
    "ENV", "ENVY", "EPS", "ES", "ESE", "ETC", "FIN", "FIT", "FOR", "FRE", "FW",
    "FYS", "GCS", "GER", "GLG", "GRK", "GSP", "HA", "HHS", "HIS", "HISY", "HON",
    "HPI", "HS", "HUM", "IBH", "ICJ", "ID", "IH", "INF", "INT", "ISM", "ITA",
    "ITG", "JLS", "JPN", "JUS", "LAN", "LAS", "LAT", "LEA", "MAT", "ME", "MER",
    "MGBA", "MGT", "MKT", "MOL", "MS", "MST", "MUP", "MUS", "NAU", "NAUY", "NAV",
    "NSE", "NTS", "NUR", "OTD", "PADM", "PE", "PHA", "PHI", "PHO", "PHS", "PHY",
    "POR", "PM", "POS", "PR", "PRM", "PSY", "PT", "REL", "RUS", "SA", "SBA",
    "SBS", "SE", "SIMY", "SOC", "SOCIO", "SPA", "SST", "STA", "STR", "SUS", "SW",
    "SYS", "TH", "TRAN", "TSM", "USC", "VC", "WGS",
]

# =========================
# DATA MODEL
# =========================


@dataclass
class Course:
    """A dataclass to represent a single course."""
    term: str
    catalog_year: Optional[str]
    prefix: str
    number: str
    title: str
    description: Optional[str]
    units: Optional[str]
    sections_offered: Optional[str]
    url: str


# =========================
# UTILS
# =========================


def polite_sleep():
    """Waits for a short period to avoid overwhelming the server."""
    time.sleep(SLEEP_TIME)


def make_driver() -> WebDriver:
    """
    Initializes and returns a Selenium WebDriver instance.

    Configures the driver to run in headless mode if specified and sets a
    default window size.

    Returns:
        WebDriver: The configured Selenium WebDriver instance.
    """
    opts = webdriver.ChromeOptions()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--window-size=1400,900")
    return webdriver.Chrome(options=opts)


def results_url(prefix: str, term_code: int) -> str:
    """
    Constructs the URL for the course results page for a given prefix and term.

    Args:
        prefix (str): The course prefix (e.g., "ACC").
        term_code (int): The internal term code for the academic term.

    Returns:
        str: The fully-qualified URL.
    """
    return f"{BASE}/results?subject={prefix}&catNbr=&term={term_code}"


def load_existing_courses() -> Dict[str, Dict]:
    """
    Loads courses from the CSV file into a dictionary keyed by course URL.

    This allows the script to resume and avoid re-scraping existing data.

    Returns:
        Dict[str, Dict]: A dictionary mapping course URLs to a dictionary
                         of course data. Returns an empty dict if the file
                         does not exist.
    """
    existing = {}
    try:
        with open(CSV_PATH, newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing[row["url"]] = row
        print(f"Loaded {len(existing)} existing courses from {CSV_PATH}")
    except FileNotFoundError:
        print(f"No existing CSV found at {CSV_PATH} â€” starting fresh.")
    return existing


def write_csv(rows: Dict[str, Dict]):
    """
    Writes the given course data to the main CSV file.

    This function overwrites the entire file to ensure consistency.

    Args:
        rows (Dict[str, Dict]): A dictionary of course data, where keys are
                                course URLs.
    """
    if not rows:
        return

    # The fieldnames are derived from the first item in the dictionary.
    fieldnames = rows[next(iter(rows))].keys()

    with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows.values())


def log_empty_prefix(term_label: str, term_code: int, prefix: str, error: str):
    """
    Logs a course prefix that returned no results to a separate CSV file.

    Args:
        term_label (str): The human-readable academic term.
        term_code (int): The internal term code.
        prefix (str): The course prefix that was empty.
        error (str): A short description of why it's considered empty.
    """
    # Create the file and write the header if it doesn't exist.
    write_header = not os.path.exists(EMPTY_PREFIXES_CSV)
    with open(EMPTY_PREFIXES_CSV, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f, fieldnames=["term", "term_code", "prefix", "error"]
        )
        if write_header:
            writer.writeheader()
        writer.writerow(
            {
                "term": term_label,
                "term_code": term_code,
                "prefix": prefix,
                "error": error,
            }
        )


# =========================
# SCRAPING HELPERS
# =========================


def get_course_links(
    driver: WebDriver, prefix: str, term_code: int, retries: int = 2, wait_s: int = 15
) -> List[str]:
    """
    Fetches the list of individual course page URLs for a given prefix and term.

    Args:
        driver (WebDriver): The Selenium driver.
        prefix (str): The course prefix.
        term_code (int): The internal term code.
        retries (int): The number of times to retry on timeout.
        wait_s (int): The number of seconds for explicit waits.

    Returns:
        List[str]: A sorted list of unique course page URLs.
    """
    for attempt in range(retries + 1):
        driver.get(results_url(prefix, term_code))
        wait = WebDriverWait(driver, wait_s)

        try:
            # Wait for either the course list or the "no courses found" message.
            wait.until(
                EC.any_of(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "dl#results-list dt.result-item > a")
                    ),
                    EC.presence_of_element_located(
                        (By.XPATH, "//div[@id='main']//h1[normalize-space()='No courses found']")
                    ),
                )
            )
        except TimeoutException:
            if attempt < retries:
                print(f"[WARN] Timeout on {prefix} list page, retrying...")
                continue
            print(f"[WARN] {prefix} list timed out after {retries} retries; skipping.")
            return []

        # If the "no courses found" message is present, return an empty list.
        if driver.find_elements(By.XPATH, "//div[@id='main']//h1[normalize-space()='No courses found']"):
            return []

        # Collect all unique course links from the page.
        links = set()
        for a in driver.find_elements(By.CSS_SELECTOR, "dl#results-list dt.result-item > a"):
            href = a.get_attribute("href")
            if href:
                # Ensure the URL is absolute.
                if href.startswith("course?"):
                    href = f"{BASE}/{href}"
                links.add(href)

        return sorted(list(links))

    return []


def text_after_label(driver: WebDriver, label: str) -> Optional[str]:
    """
    Extracts the text content immediately following a bolded label element.

    This is used to get fields like "Description:" or "Units:".

    Args:
        driver (WebDriver): The Selenium driver.
        label (str): The text of the label to find (e.g., "Description").

    Returns:
        Optional[str]: The text content, or None if the label isn't found.
    """
    try:
        strong_element = driver.find_element(
            By.XPATH,
            f"//div[@id='courseResults']//strong[normalize-space()='{label}:']",
        )
    except Exception:
        return None

    # Use JavaScript to get the next sibling text node's content.
    # This is more robust for handling variations in spacing and nested elements.
    try:
        return driver.execute_script(
            """
            let node = arguments[0].nextSibling;
            while (node) {
              const text = (node.textContent || '').trim();
              if (text) return text;
              node = node.nextSibling;
            }
            return null;
            """,
            strong_element,
        )
    except Exception:
        return None


def get_catalog_year(driver: WebDriver) -> Optional[str]:
    """
    Extracts the catalog year from the course page header.

    Args:
        driver (WebDriver): The Selenium driver.

    Returns:
        Optional[str]: The catalog year (e.g., "2023-2024"), or None.
    """
    try:
        header_text = driver.find_element(By.CSS_SELECTOR, "#h1-first").text
        match = re.search(r"Catalog Year\s*:\s*([0-9]{4}\s*-\s*[0-9]{4})", header_text)
        if match:
            return match.group(1).replace(" ", "")
    except Exception:
        pass
    return None


def get_sections_offered(driver: WebDriver) -> Optional[str]:
    """
    Extracts the "Sections offered" links from the course page.

    Args:
        driver (WebDriver): The Selenium driver.

    Returns:
        Optional[str]: A semicolon-separated string of section terms, or None.
    """
    try:
        anchors = driver.find_elements(
            By.XPATH,
            "//div[@id='courseResults']//strong[normalize-space()='Sections offered:']/following-sibling::a",
        )
        texts = [a.text.strip() for a in anchors if a.text.strip()]
        return "; ".join(texts) if texts else None
    except Exception:
        return None


def scrape_course(driver: WebDriver, url: str, term_label: str) -> Course:
    """
    Scrapes all relevant details from a single course page.

    Args:
        driver (WebDriver): The Selenium driver.
        url (str): The URL of the course page to scrape.
        term_label (str): The human-readable academic term for this scrape.

    Returns:
        Course: A `Course` dataclass instance with the scraped information.
    """
    driver.get(url)
    wait = WebDriverWait(driver, 15)

    # The main header contains the prefix, number, and title.
    h2 = wait.until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "#courseResults h2"))
    )
    header = h2.text.strip()

    # Regex to parse "PREFIX 123 - Course Title"
    match = re.match(r"^([A-Z&]{2,6})\s+(\d{3}[A-Z]?)\s*-\s*(.+)$", header)
    prefix, number, title = ("", "", header)
    if match:
        prefix, number, title = match.group(1), match.group(2), match.group(3).strip()

    course = Course(
        term=term_label,
        catalog_year=get_catalog_year(driver),
        prefix=prefix,
        number=number,
        title=title,
        description=text_after_label(driver, "Description"),
        units=text_after_label(driver, "Units"),
        sections_offered=get_sections_offered(driver),
        url=url,
    )

    polite_sleep()
    return course


# =========================
# MAIN
# =========================


def main():
    """
    The main execution function for the scraper.

    Initializes the driver, loads existing data, iterates through terms and
    prefixes, scrapes new courses, and saves the results.
    """
    existing = load_existing_courses()
    driver = make_driver()

    try:
        total_prefixes = len(PREFIXES) * len(TERM_CODES)
        step = 1

        for term_label, term_code in TERM_CODES.items():
            for prefix in PREFIXES:
                print(
                    f"[{step}/{total_prefixes}] {term_label} {prefix}: fetching list..."
                )
                links = get_course_links(driver, prefix, term_code)
                print(
                    f"[{step}/{total_prefixes}] {term_label} {prefix}: {len(links)} courses found"
                )

                if not links:
                    log_empty_prefix(term_label, term_code, prefix, "empty")
                    step += 1
                    continue

                new_count = 0
                for link in links:
                    # Skip if we already have this course and are not in overwrite mode.
                    if link in existing and not OVERWRITE:
                        continue

                    try:
                        course = scrape_course(driver, link, term_label)
                        existing[link] = asdict(course)
                        new_count += 1
                    except Exception as e:
                        print(f"[WARN] Failed to scrape {link}: {e}")

                if new_count > 0:
                    print(
                        f"[{step}/{total_prefixes}] {term_label} {prefix}: Scraped {new_count} new/updated courses"
                    )
                    # Write after each prefix is complete as a safeguard.
                    write_csv(existing)

                step += 1

    finally:
        print("Scraping finished. Shutting down WebDriver.")
        driver.quit()

    print(f"Finished. Total courses in CSV: {len(existing)}")


if __name__ == "__main__":
    main()
